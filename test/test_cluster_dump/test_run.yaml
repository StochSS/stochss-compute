scheduler:
  address: tcp://127.0.0.1:43613
  clients:
    Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1:
      client_key: Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1
      last_seen: 1675201285.3166108
      wants_what: []
    fire-and-forget:
      client_key: fire-and-forget
      last_seen: 1675201285.2746866
      wants_what: []
  events:
    Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1:
    - - 1675201285.3165696
      - action: add-client
        client: Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1
    all:
    - - 1675201285.2964067
      - action: add-worker
        worker: tcp://127.0.0.1:38211
    - - 1675201285.2978168
      - action: add-worker
        worker: tcp://127.0.0.1:43901
    - - 1675201285.3165696
      - action: add-client
        client: Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1
    stealing: []
    tcp://127.0.0.1:38211:
    - - 1675201285.296389
      - action: add-worker
    - - 1675201285.3010023
      - action: worker-status-change
        prev-status: init
        status: running
    tcp://127.0.0.1:43901:
    - - 1675201285.2978065
      - action: add-worker
    - - 1675201285.3011415
      - action: worker-status-change
        prev-status: init
        status: running
  extensions:
    amm: <distributed.active_memory_manager.ActiveMemoryManagerExtension object at
      0x7f7ef5cd6b90>
    events: <distributed.event.EventExtension object at 0x7f7ef5cd6b10>
    locks: <distributed.lock.LockExtension object at 0x7f7ef5cc46d0>
    memory_sampler: <distributed.diagnostics.memory_sampler.MemorySamplerExtension
      object at 0x7f7ef67d8b90>
    multi_locks: <distributed.multi_lock.MultiLockExtension object at 0x7f7ef5cc4610>
    publish: <distributed.publish.PublishExtension object at 0x7f7ef5cd5d50>
    pubsub: <distributed.pubsub.PubSubSchedulerExtension object at 0x7f7ef5cd6490>
    queues: <distributed.queues.QueueExtension object at 0x7f7ef5cd5f90>
    replay-tasks: <distributed.recreate_tasks.ReplayTaskScheduler object at 0x7f7ef5cd5ed0>
    semaphores: <distributed.semaphore.SemaphoreExtension object at 0x7f7ef5cd6610>
    shuffle: <distributed.shuffle._scheduler_extension.ShuffleSchedulerExtension object
      at 0x7f7ef5cd6e90>
    stealing:
      cost_multipliers:
      - 1.0
      - 1.03125
      - 1.0625
      - 1.125
      - 1.25
      - 1.5
      - 2
      - 3
      - 5
      - 9
      - 17
      - 33
      - 65
      - 129
      - 257
      count: 0
      in_flight: {}
      in_flight_occupancy: {}
      in_flight_tasks: {}
      key_stealable: {}
      metrics:
        request_cost_total: {}
        request_count_total: {}
      scheduler:
        address: tcp://127.0.0.1:43613
        clients:
          Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1: <Client 'Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1'>
          fire-and-forget: <Client 'fire-and-forget'>
        events:
          Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1:
          - - 1675201285.3165696
            - action: add-client
              client: Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1
          all:
          - - 1675201285.2964067
            - action: add-worker
              worker: tcp://127.0.0.1:38211
          - - 1675201285.2978168
            - action: add-worker
              worker: tcp://127.0.0.1:43901
          - - 1675201285.3165696
            - action: add-client
              client: Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1
          stealing: []
          tcp://127.0.0.1:38211:
          - - 1675201285.296389
            - action: add-worker
          - - 1675201285.3010023
            - action: worker-status-change
              prev-status: init
              status: running
          tcp://127.0.0.1:43901:
          - - 1675201285.2978065
            - action: add-worker
          - - 1675201285.3011415
            - action: worker-status-change
              prev-status: init
              status: running
        extensions: '{''locks'': <distributed.lock.LockExtension object at 0x7f7ef5cc46d0>,
          ''multi_locks'': <distributed.multi_lock.MultiLockExtension object at 0x7f7ef5cc4610>,
          ''publish'': <distributed.publish.PublishExtension object at 0x7f7ef5cd5d50>,
          ''replay-tasks'': <distributed.recreate_tasks.ReplayTaskScheduler object
          at 0x7f7ef5cd5ed0>, ''queues'': <distributed.queues.QueueExtension object
          at 0x7f7ef5cd5f90>, ''variables'': <distributed.variable.VariableExtension
          object at 0x7f7ef5cd61d0>, ''pubsub'': <distributed.pubsub.PubSubSchedulerExtension
          object at 0x7f7ef5cd6490>, ''semaphores'': <distributed.semaphore.SemaphoreExtension
          object at 0x7f7ef5cd6610>, ''events'': <distributed.event.EventExtension
          object at 0x7f7ef5cd6b10>, ''amm'': <distributed.active_memory_manager.ActiveMemoryManagerExtension
          object at 0x7f7ef5cd6b90>, ''memory_sampler'': <distributed.diagnostics.memory_sampler.MemorySamplerExtension
          object at 0x7f7ef67d8b90>, ''shuffle'': <distributed.shuffle._scheduler_extension.ShuffleSchedulerExtension
          object at 0x7f7ef5cd6e90>, ''stealing'': <distributed.stealing.WorkStealing
          object at 0x7f7ef5cd7010>}'
        id: Scheduler-fd3a9a23-ffb8-4550-b3a8-d3117d985a3f
        memory:
          managed: 0
          managed_in_memory: 0
          managed_spilled: 0
          optimistic: 270680064
          process: 270680064
          unmanaged: 270680064
          unmanaged_old: 270680064
          unmanaged_recent: 0
        services:
          dashboard: 43515
        started: 1675201285.2586555
        status: running
        task_groups: {}
        tasks: {}
        thread_id: 140184394814080
        transition_counter: 0
        transition_log: []
        type: Scheduler
        workers:
          tcp://127.0.0.1:38211: '<WorkerState ''tcp://127.0.0.1:38211'', name: 0,
            status: running, memory: 0, processing: 0>'
          tcp://127.0.0.1:43901: '<WorkerState ''tcp://127.0.0.1:43901'', name: 1,
            status: running, memory: 0, processing: 0>'
      stealable:
        tcp://127.0.0.1:38211:
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        tcp://127.0.0.1:43901:
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
    variables: <distributed.variable.VariableExtension object at 0x7f7ef5cd61d0>
  id: Scheduler-fd3a9a23-ffb8-4550-b3a8-d3117d985a3f
  memory:
    managed: 0
    managed_in_memory: 0
    managed_spilled: 0
    optimistic: 270680064
    process: 270680064
    unmanaged: 270680064
    unmanaged_old: 270680064
    unmanaged_recent: 0
  services:
    dashboard: 43515
  started: 1675201285.2586555
  status: running
  task_groups: {}
  tasks: {}
  thread_id: 140184394814080
  transition_counter: 0
  transition_log: []
  type: Scheduler
  workers:
    tcp://127.0.0.1:38211:
      actors: []
      address: tcp://127.0.0.1:38211
      bandwidth: 100000000
      executing: {}
      extra: {}
      has_what: []
      host: 127.0.0.1
      last_seen: 1675201285.2964714
      local_directory: /tmp/dask-worker-space/worker-vf45xpkt
      long_running: []
      memory:
        managed: 0
        managed_in_memory: 0
        managed_spilled: 0
        optimistic: 135340032
        process: 135340032
        unmanaged: 135340032
        unmanaged_old: 135340032
        unmanaged_recent: 0
      memory_limit: 16371576832
      metrics:
        bandwidth:
          total: 100000000
          types: {}
          workers: {}
        cpu: 0.0
        event_loop_interval: 0.5
        host_disk_io:
          read_bps: 0.0
          write_bps: 0.0
        host_net_io:
          read_bps: 0.0
          write_bps: 0.0
        managed_bytes: 0
        memory: 135340032
        num_fds: 18
        spilled_bytes:
          disk: 0
          memory: 0
        task_counts:
          constrained: 0
          executing: 0
          fetch: 0
          flight: 0
          long-running: 0
          memory: 0
          missing: 0
          other: 0
          ready: 0
          waiting: 0
        time: 1675201285.281705
        transfer:
          incoming_bytes: 0
          incoming_count: 0
          incoming_count_total: 0
          outgoing_bytes: 0
          outgoing_count: 0
          outgoing_count_total: 0
      name: 0
      nanny: null
      nbytes: 0
      needs_what: {}
      nthreads: 1
      occupancy: 0.0
      pid: 352440
      processing: []
      resources: {}
      scheduler:
        address: tcp://127.0.0.1:43613
        clients:
          Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1: <Client 'Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1'>
          fire-and-forget: <Client 'fire-and-forget'>
        events:
          Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1:
          - - 1675201285.3165696
            - action: add-client
              client: Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1
          all:
          - - 1675201285.2964067
            - action: add-worker
              worker: tcp://127.0.0.1:38211
          - - 1675201285.2978168
            - action: add-worker
              worker: tcp://127.0.0.1:43901
          - - 1675201285.3165696
            - action: add-client
              client: Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1
          stealing: []
          tcp://127.0.0.1:38211:
          - - 1675201285.296389
            - action: add-worker
          - - 1675201285.3010023
            - action: worker-status-change
              prev-status: init
              status: running
          tcp://127.0.0.1:43901:
          - - 1675201285.2978065
            - action: add-worker
          - - 1675201285.3011415
            - action: worker-status-change
              prev-status: init
              status: running
        extensions:
          amm: <distributed.active_memory_manager.ActiveMemoryManagerExtension object
            at 0x7f7ef5cd6b90>
          events: <distributed.event.EventExtension object at 0x7f7ef5cd6b10>
          locks: <distributed.lock.LockExtension object at 0x7f7ef5cc46d0>
          memory_sampler: <distributed.diagnostics.memory_sampler.MemorySamplerExtension
            object at 0x7f7ef67d8b90>
          multi_locks: <distributed.multi_lock.MultiLockExtension object at 0x7f7ef5cc4610>
          publish: <distributed.publish.PublishExtension object at 0x7f7ef5cd5d50>
          pubsub: <distributed.pubsub.PubSubSchedulerExtension object at 0x7f7ef5cd6490>
          queues: <distributed.queues.QueueExtension object at 0x7f7ef5cd5f90>
          replay-tasks: <distributed.recreate_tasks.ReplayTaskScheduler object at
            0x7f7ef5cd5ed0>
          semaphores: <distributed.semaphore.SemaphoreExtension object at 0x7f7ef5cd6610>
          shuffle: <distributed.shuffle._scheduler_extension.ShuffleSchedulerExtension
            object at 0x7f7ef5cd6e90>
          stealing: <distributed.stealing.WorkStealing object at 0x7f7ef5cd7010>
          variables: <distributed.variable.VariableExtension object at 0x7f7ef5cd61d0>
        id: Scheduler-fd3a9a23-ffb8-4550-b3a8-d3117d985a3f
        memory:
          managed: 0
          managed_in_memory: 0
          managed_spilled: 0
          optimistic: 270680064
          process: 270680064
          unmanaged: 270680064
          unmanaged_old: 270680064
          unmanaged_recent: 0
        services:
          dashboard: 43515
        started: 1675201285.2586555
        status: running
        task_groups: {}
        tasks: {}
        thread_id: 140184394814080
        transition_counter: 0
        transition_log: []
        type: Scheduler
        workers: 'SortedDict({''tcp://127.0.0.1:38211'': <WorkerState ''tcp://127.0.0.1:38211'',
          name: 0, status: running, memory: 0, processing: 0>, ''tcp://127.0.0.1:43901'':
          <WorkerState ''tcp://127.0.0.1:43901'', name: 1, status: running, memory:
          0, processing: 0>})'
      server_id: Worker-07a44a56-32c9-4cc1-b293-587b3df5be2f
      services:
        dashboard: 42715
      status: '<Status.running: ''running''>'
      task_prefix_count: {}
      time_delay: 0.004288911819458008
      used_resources: {}
    tcp://127.0.0.1:43901:
      actors: []
      address: tcp://127.0.0.1:43901
      bandwidth: 100000000
      executing: {}
      extra: {}
      has_what: []
      host: 127.0.0.1
      last_seen: 1675201285.297859
      local_directory: /tmp/dask-worker-space/worker-suxu10di
      long_running: []
      memory:
        managed: 0
        managed_in_memory: 0
        managed_spilled: 0
        optimistic: 135340032
        process: 135340032
        unmanaged: 135340032
        unmanaged_old: 135340032
        unmanaged_recent: 0
      memory_limit: 16371576832
      metrics:
        bandwidth:
          total: 100000000
          types: {}
          workers: {}
        cpu: 0.0
        event_loop_interval: 0.5
        host_disk_io:
          read_bps: 0.0
          write_bps: 0.0
        host_net_io:
          read_bps: 0.0
          write_bps: 0.0
        managed_bytes: 0
        memory: 135340032
        num_fds: 19
        spilled_bytes:
          disk: 0
          memory: 0
        task_counts:
          constrained: 0
          executing: 0
          fetch: 0
          flight: 0
          long-running: 0
          memory: 0
          missing: 0
          other: 0
          ready: 0
          waiting: 0
        time: 1675201285.284868
        transfer:
          incoming_bytes: 0
          incoming_count: 0
          incoming_count_total: 0
          outgoing_bytes: 0
          outgoing_count: 0
          outgoing_count_total: 0
      name: 1
      nanny: null
      nbytes: 0
      needs_what: {}
      nthreads: 2
      occupancy: 0.0
      pid: 352440
      processing: []
      resources: {}
      scheduler:
        address: tcp://127.0.0.1:43613
        clients:
          Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1: <Client 'Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1'>
          fire-and-forget: <Client 'fire-and-forget'>
        events:
          Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1:
          - - 1675201285.3165696
            - action: add-client
              client: Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1
          all:
          - - 1675201285.2964067
            - action: add-worker
              worker: tcp://127.0.0.1:38211
          - - 1675201285.2978168
            - action: add-worker
              worker: tcp://127.0.0.1:43901
          - - 1675201285.3165696
            - action: add-client
              client: Client-02e347f5-a1b0-11ed-a0b8-af5c30316ec1
          stealing: []
          tcp://127.0.0.1:38211:
          - - 1675201285.296389
            - action: add-worker
          - - 1675201285.3010023
            - action: worker-status-change
              prev-status: init
              status: running
          tcp://127.0.0.1:43901:
          - - 1675201285.2978065
            - action: add-worker
          - - 1675201285.3011415
            - action: worker-status-change
              prev-status: init
              status: running
        extensions:
          amm: <distributed.active_memory_manager.ActiveMemoryManagerExtension object
            at 0x7f7ef5cd6b90>
          events: <distributed.event.EventExtension object at 0x7f7ef5cd6b10>
          locks: <distributed.lock.LockExtension object at 0x7f7ef5cc46d0>
          memory_sampler: <distributed.diagnostics.memory_sampler.MemorySamplerExtension
            object at 0x7f7ef67d8b90>
          multi_locks: <distributed.multi_lock.MultiLockExtension object at 0x7f7ef5cc4610>
          publish: <distributed.publish.PublishExtension object at 0x7f7ef5cd5d50>
          pubsub: <distributed.pubsub.PubSubSchedulerExtension object at 0x7f7ef5cd6490>
          queues: <distributed.queues.QueueExtension object at 0x7f7ef5cd5f90>
          replay-tasks: <distributed.recreate_tasks.ReplayTaskScheduler object at
            0x7f7ef5cd5ed0>
          semaphores: <distributed.semaphore.SemaphoreExtension object at 0x7f7ef5cd6610>
          shuffle: <distributed.shuffle._scheduler_extension.ShuffleSchedulerExtension
            object at 0x7f7ef5cd6e90>
          stealing: <distributed.stealing.WorkStealing object at 0x7f7ef5cd7010>
          variables: <distributed.variable.VariableExtension object at 0x7f7ef5cd61d0>
        id: Scheduler-fd3a9a23-ffb8-4550-b3a8-d3117d985a3f
        memory:
          managed: 0
          managed_in_memory: 0
          managed_spilled: 0
          optimistic: 270680064
          process: 270680064
          unmanaged: 270680064
          unmanaged_old: 270680064
          unmanaged_recent: 0
        services:
          dashboard: 43515
        started: 1675201285.2586555
        status: running
        task_groups: {}
        tasks: {}
        thread_id: 140184394814080
        transition_counter: 0
        transition_log: []
        type: Scheduler
        workers: 'SortedDict({''tcp://127.0.0.1:38211'': <WorkerState ''tcp://127.0.0.1:38211'',
          name: 0, status: running, memory: 0, processing: 0>, ''tcp://127.0.0.1:43901'':
          <WorkerState ''tcp://127.0.0.1:43901'', name: 1, status: running, memory:
          0, processing: 0>})'
      server_id: Worker-f312372f-b256-4468-9660-2fc67db1600d
      services:
        dashboard: 44723
      status: '<Status.running: ''running''>'
      task_prefix_count: {}
      time_delay: 0.002946615219116211
      used_resources: {}
versions:
  host:
    LANG: en_US.UTF-8
    LC_ALL: None
    OS: Linux
    OS-release: 5.15.0-58-generic
    byteorder: little
    machine: x86_64
    processor: x86_64
    python: 3.11.1.final.0
    python-bits: 64
  packages:
    cloudpickle: 2.2.0
    dask: 2022.12.1
    distributed: 2022.12.1
    lz4: null
    msgpack: 1.0.4
    numpy: 1.24.1
    pandas: null
    python: 3.11.1.final.0
    toolz: 0.12.0
    tornado: '6.2'
workers:
  tcp://127.0.0.1:38211:
    address: tcp://127.0.0.1:38211
    busy_workers: []
    config:
      array:
        backend: numpy
        chunk-size: 128MiB
        rechunk-threshold: 4
        slicing:
          split-large-chunks: null
        svg:
          size: 120
      dataframe:
        backend: pandas
        dtype_backend: pandas
        parquet:
          metadata-task-size-local: 512
          metadata-task-size-remote: 16
        shuffle-compression: null
      distributed:
        adaptive:
          interval: 1s
          maximum: .inf
          minimum: 0
          target-duration: 5s
          wait-count: 3
        admin:
          event-loop: tornado
          log-format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          log-length: 10000
          max-error-length: 10000
          pdb-on-err: false
          system-monitor:
            disk: true
            host-cpu: false
            interval: 500ms
          tick:
            cycle: 1s
            interval: 500 ms
            limit: 3s
        client:
          heartbeat: 5s
          preload: []
          preload-argv: []
          scheduler-info-interval: 2s
          security-loader: null
        comm:
          compression: auto
          default-scheme: tcp
          offload: 10MiB
          recent-messages-log-length: 0
          require-encryption: null
          retry:
            count: 0
            delay:
              max: 20s
              min: 1s
          shard: 64MiB
          socket-backlog: 2048
          tcp:
            backend: tornado
          timeouts:
            connect: 5s
            tcp: 30s
          tls:
            ca-file: null
            ciphers: null
            client:
              cert: null
              key: null
            max-version: null
            min-version: 1.2
            scheduler:
              cert: null
              key: null
            worker:
              cert: null
              key: null
          ucx:
            create-cuda-context: null
            cuda-copy: null
            environment: {}
            infiniband: null
            nvlink: null
            rdmacm: null
            tcp: null
          websockets:
            shard: 8MiB
          zstd:
            level: 3
            threads: 0
        dashboard:
          export-tool: false
          graph-max-items: 5000
          link: '{scheme}://{host}:{port}/status'
          prometheus:
            namespace: dask
        deploy:
          cluster-repr-interval: 500ms
          lost-worker-timeout: 15s
        diagnostics:
          computations:
            ignore-modules:
            - distributed
            - dask
            - xarray
            - cudf
            - cuml
            - prefect
            - xgboost
            max-history: 100
          erred-tasks:
            max-history: 100
          nvml: true
        nanny:
          environ: {}
          pre-spawn-environ:
            MALLOC_TRIM_THRESHOLD_: 65536
            MKL_NUM_THREADS: 1
            OMP_NUM_THREADS: 1
            OPENBLAS_NUM_THREADS: 1
          preload: []
          preload-argv: []
        rmm:
          pool-size: null
        scheduler:
          active-memory-manager:
            interval: 2s
            measure: optimistic
            policies:
            - class: distributed.active_memory_manager.ReduceReplicas
            start: true
          allowed-failures: 3
          allowed-imports:
          - dask
          - distributed
          bandwidth: 100000000
          blocked-handlers: []
          contact-address: null
          dashboard:
            bokeh-application:
              allow_websocket_origin:
              - '*'
              check_unused_sessions_milliseconds: 500
              keep_alive_milliseconds: 500
            status:
              task-stream-length: 1000
            tasks:
              task-stream-length: 100000
            tls:
              ca-file: null
              cert: null
              key: null
          default-data-size: 1kiB
          default-task-durations:
            rechunk-split: 1us
            split-shuffle: 1us
          events-cleanup-delay: 1h
          events-log-length: 100000
          http:
            routes:
            - distributed.http.scheduler.prometheus
            - distributed.http.scheduler.info
            - distributed.http.scheduler.json
            - distributed.http.health
            - distributed.http.proxy
            - distributed.http.statics
          idle-timeout: null
          locks:
            lease-timeout: 30s
            lease-validation-interval: 10s
          pickle: true
          preload: []
          preload-argv: []
          transition-log-length: 100000
          unknown-task-duration: 500ms
          validate: false
          work-stealing: true
          work-stealing-interval: 100ms
          worker-saturation: 1.1
          worker-ttl: 5 minutes
        version: 2
        worker:
          blocked-handlers: []
          connections:
            incoming: 10
            outgoing: 50
          daemon: true
          http:
            routes:
            - distributed.http.worker.prometheus
            - distributed.http.health
            - distributed.http.statics
          lifetime:
            duration: null
            restart: false
            stagger: 0 seconds
          memory:
            max-spill: false
            monitor-interval: 100ms
            pause: 0.8
            rebalance:
              measure: optimistic
              recipient-max: 0.6
              sender-min: 0.3
              sender-recipient-gap: 0.1
            recent-to-old-time: 30s
            spill: 0.7
            target: 0.6
            terminate: 0.95
            transfer: 0.1
          multiprocessing-method: spawn
          preload: []
          preload-argv: []
          profile:
            cycle: 1000ms
            enabled: false
            interval: 10ms
            low-level: false
          resources: {}
          transfer:
            message-bytes-limit: 50MB
          use-file-locking: true
          validate: false
      local_directory: /tmp
      optimization:
        annotations:
          fuse: true
        fuse:
          active: null
          ave-width: 1
          max-depth-new-edges: null
          max-height: .inf
          max-width: null
          rename-keys: true
          subgraphs: null
      scheduler: dask.distributed
      shuffle: tasks
      temporary-directory: null
      tokenize:
        ensure-deterministic: false
      visualization:
        engine: null
    constrained: []
    data: {}
    data_needed: {}
    executing: []
    has_what: {}
    id: Worker-07a44a56-32c9-4cc1-b293-587b3df5be2f
    in_flight_tasks: []
    in_flight_workers: {}
    log: []
    logs:
    - - INFO
      - '2023-01-31 16:41:25,287 - distributed.worker - INFO -       Start worker
        at:      tcp://127.0.0.1:38211'
    - - INFO
      - '2023-01-31 16:41:25,287 - distributed.worker - INFO -          Listening
        to:      tcp://127.0.0.1:38211'
    - - INFO
      - '2023-01-31 16:41:25,287 - distributed.worker - INFO -           Worker name:                          0'
    - - INFO
      - '2023-01-31 16:41:25,287 - distributed.worker - INFO -          dashboard
        at:            127.0.0.1:42715'
    - - INFO
      - '2023-01-31 16:41:25,287 - distributed.worker - INFO - Waiting to connect
        to:      tcp://127.0.0.1:43613'
    - - INFO
      - 2023-01-31 16:41:25,287 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-31 16:41:25,287 - distributed.worker - INFO -               Threads:                          1'
    - - INFO
      - '2023-01-31 16:41:25,288 - distributed.worker - INFO -                Memory:                  15.25
        GiB'
    - - INFO
      - '2023-01-31 16:41:25,288 - distributed.worker - INFO -       Local Directory:
        /tmp/dask-worker-space/worker-vf45xpkt'
    - - INFO
      - 2023-01-31 16:41:25,288 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-31 16:41:25,288 - distributed.worker - INFO -       Start worker
        at:      tcp://127.0.0.1:43901'
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO -          Listening
        to:      tcp://127.0.0.1:43901'
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO -           Worker name:                          1'
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO -          dashboard
        at:            127.0.0.1:44723'
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO - Waiting to connect
        to:      tcp://127.0.0.1:43613'
    - - INFO
      - 2023-01-31 16:41:25,289 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO -               Threads:                          2'
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO -                Memory:                  15.25
        GiB'
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO -       Local Directory:
        /tmp/dask-worker-space/worker-suxu10di'
    - - INFO
      - 2023-01-31 16:41:25,289 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-31 16:41:25,299 - distributed.worker - INFO -         Registered
        to:      tcp://127.0.0.1:43613'
    - - INFO
      - 2023-01-31 16:41:25,299 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-31 16:41:25,299 - distributed.worker - INFO -         Registered
        to:      tcp://127.0.0.1:43613'
    - - INFO
      - 2023-01-31 16:41:25,299 - distributed.worker - INFO - -------------------------------------------------
    long_running: []
    max_spill: false
    memory_limit: 16371576832
    memory_monitor_interval: 0.1
    memory_pause_fraction: 0.8
    memory_spill_fraction: 0.7
    memory_target_fraction: 0.6
    missing_dep_flight: []
    nthreads: 1
    ready: []
    running: true
    scheduler: tcp://127.0.0.1:43613
    status: '<Status.running: ''running''>'
    stimulus_log: []
    tasks: {}
    thread_id: 140184394814080
    transfer_incoming_log: []
    transfer_outgoing_log: []
    transition_counter: 0
    type: Worker
  tcp://127.0.0.1:43901:
    address: tcp://127.0.0.1:43901
    busy_workers: []
    config:
      array:
        backend: numpy
        chunk-size: 128MiB
        rechunk-threshold: 4
        slicing:
          split-large-chunks: null
        svg:
          size: 120
      dataframe:
        backend: pandas
        dtype_backend: pandas
        parquet:
          metadata-task-size-local: 512
          metadata-task-size-remote: 16
        shuffle-compression: null
      distributed:
        adaptive:
          interval: 1s
          maximum: .inf
          minimum: 0
          target-duration: 5s
          wait-count: 3
        admin:
          event-loop: tornado
          log-format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          log-length: 10000
          max-error-length: 10000
          pdb-on-err: false
          system-monitor:
            disk: true
            host-cpu: false
            interval: 500ms
          tick:
            cycle: 1s
            interval: 500 ms
            limit: 3s
        client:
          heartbeat: 5s
          preload: []
          preload-argv: []
          scheduler-info-interval: 2s
          security-loader: null
        comm:
          compression: auto
          default-scheme: tcp
          offload: 10MiB
          recent-messages-log-length: 0
          require-encryption: null
          retry:
            count: 0
            delay:
              max: 20s
              min: 1s
          shard: 64MiB
          socket-backlog: 2048
          tcp:
            backend: tornado
          timeouts:
            connect: 5s
            tcp: 30s
          tls:
            ca-file: null
            ciphers: null
            client:
              cert: null
              key: null
            max-version: null
            min-version: 1.2
            scheduler:
              cert: null
              key: null
            worker:
              cert: null
              key: null
          ucx:
            create-cuda-context: null
            cuda-copy: null
            environment: {}
            infiniband: null
            nvlink: null
            rdmacm: null
            tcp: null
          websockets:
            shard: 8MiB
          zstd:
            level: 3
            threads: 0
        dashboard:
          export-tool: false
          graph-max-items: 5000
          link: '{scheme}://{host}:{port}/status'
          prometheus:
            namespace: dask
        deploy:
          cluster-repr-interval: 500ms
          lost-worker-timeout: 15s
        diagnostics:
          computations:
            ignore-modules:
            - distributed
            - dask
            - xarray
            - cudf
            - cuml
            - prefect
            - xgboost
            max-history: 100
          erred-tasks:
            max-history: 100
          nvml: true
        nanny:
          environ: {}
          pre-spawn-environ:
            MALLOC_TRIM_THRESHOLD_: 65536
            MKL_NUM_THREADS: 1
            OMP_NUM_THREADS: 1
            OPENBLAS_NUM_THREADS: 1
          preload: []
          preload-argv: []
        rmm:
          pool-size: null
        scheduler:
          active-memory-manager:
            interval: 2s
            measure: optimistic
            policies:
            - class: distributed.active_memory_manager.ReduceReplicas
            start: true
          allowed-failures: 3
          allowed-imports:
          - dask
          - distributed
          bandwidth: 100000000
          blocked-handlers: []
          contact-address: null
          dashboard:
            bokeh-application:
              allow_websocket_origin:
              - '*'
              check_unused_sessions_milliseconds: 500
              keep_alive_milliseconds: 500
            status:
              task-stream-length: 1000
            tasks:
              task-stream-length: 100000
            tls:
              ca-file: null
              cert: null
              key: null
          default-data-size: 1kiB
          default-task-durations:
            rechunk-split: 1us
            split-shuffle: 1us
          events-cleanup-delay: 1h
          events-log-length: 100000
          http:
            routes:
            - distributed.http.scheduler.prometheus
            - distributed.http.scheduler.info
            - distributed.http.scheduler.json
            - distributed.http.health
            - distributed.http.proxy
            - distributed.http.statics
          idle-timeout: null
          locks:
            lease-timeout: 30s
            lease-validation-interval: 10s
          pickle: true
          preload: []
          preload-argv: []
          transition-log-length: 100000
          unknown-task-duration: 500ms
          validate: false
          work-stealing: true
          work-stealing-interval: 100ms
          worker-saturation: 1.1
          worker-ttl: 5 minutes
        version: 2
        worker:
          blocked-handlers: []
          connections:
            incoming: 10
            outgoing: 50
          daemon: true
          http:
            routes:
            - distributed.http.worker.prometheus
            - distributed.http.health
            - distributed.http.statics
          lifetime:
            duration: null
            restart: false
            stagger: 0 seconds
          memory:
            max-spill: false
            monitor-interval: 100ms
            pause: 0.8
            rebalance:
              measure: optimistic
              recipient-max: 0.6
              sender-min: 0.3
              sender-recipient-gap: 0.1
            recent-to-old-time: 30s
            spill: 0.7
            target: 0.6
            terminate: 0.95
            transfer: 0.1
          multiprocessing-method: spawn
          preload: []
          preload-argv: []
          profile:
            cycle: 1000ms
            enabled: false
            interval: 10ms
            low-level: false
          resources: {}
          transfer:
            message-bytes-limit: 50MB
          use-file-locking: true
          validate: false
      local_directory: /tmp
      optimization:
        annotations:
          fuse: true
        fuse:
          active: null
          ave-width: 1
          max-depth-new-edges: null
          max-height: .inf
          max-width: null
          rename-keys: true
          subgraphs: null
      scheduler: dask.distributed
      shuffle: tasks
      temporary-directory: null
      tokenize:
        ensure-deterministic: false
      visualization:
        engine: null
    constrained: []
    data: {}
    data_needed: {}
    executing: []
    has_what: {}
    id: Worker-f312372f-b256-4468-9660-2fc67db1600d
    in_flight_tasks: []
    in_flight_workers: {}
    log: []
    logs:
    - - INFO
      - '2023-01-31 16:41:25,287 - distributed.worker - INFO -       Start worker
        at:      tcp://127.0.0.1:38211'
    - - INFO
      - '2023-01-31 16:41:25,287 - distributed.worker - INFO -          Listening
        to:      tcp://127.0.0.1:38211'
    - - INFO
      - '2023-01-31 16:41:25,287 - distributed.worker - INFO -           Worker name:                          0'
    - - INFO
      - '2023-01-31 16:41:25,287 - distributed.worker - INFO -          dashboard
        at:            127.0.0.1:42715'
    - - INFO
      - '2023-01-31 16:41:25,287 - distributed.worker - INFO - Waiting to connect
        to:      tcp://127.0.0.1:43613'
    - - INFO
      - 2023-01-31 16:41:25,287 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-31 16:41:25,287 - distributed.worker - INFO -               Threads:                          1'
    - - INFO
      - '2023-01-31 16:41:25,288 - distributed.worker - INFO -                Memory:                  15.25
        GiB'
    - - INFO
      - '2023-01-31 16:41:25,288 - distributed.worker - INFO -       Local Directory:
        /tmp/dask-worker-space/worker-vf45xpkt'
    - - INFO
      - 2023-01-31 16:41:25,288 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-31 16:41:25,288 - distributed.worker - INFO -       Start worker
        at:      tcp://127.0.0.1:43901'
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO -          Listening
        to:      tcp://127.0.0.1:43901'
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO -           Worker name:                          1'
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO -          dashboard
        at:            127.0.0.1:44723'
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO - Waiting to connect
        to:      tcp://127.0.0.1:43613'
    - - INFO
      - 2023-01-31 16:41:25,289 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO -               Threads:                          2'
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO -                Memory:                  15.25
        GiB'
    - - INFO
      - '2023-01-31 16:41:25,289 - distributed.worker - INFO -       Local Directory:
        /tmp/dask-worker-space/worker-suxu10di'
    - - INFO
      - 2023-01-31 16:41:25,289 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-31 16:41:25,299 - distributed.worker - INFO -         Registered
        to:      tcp://127.0.0.1:43613'
    - - INFO
      - 2023-01-31 16:41:25,299 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-31 16:41:25,299 - distributed.worker - INFO -         Registered
        to:      tcp://127.0.0.1:43613'
    - - INFO
      - 2023-01-31 16:41:25,299 - distributed.worker - INFO - -------------------------------------------------
    long_running: []
    max_spill: false
    memory_limit: 16371576832
    memory_monitor_interval: 0.1
    memory_pause_fraction: 0.8
    memory_spill_fraction: 0.7
    memory_target_fraction: 0.6
    missing_dep_flight: []
    nthreads: 2
    ready: []
    running: true
    scheduler: tcp://127.0.0.1:43613
    status: '<Status.running: ''running''>'
    stimulus_log: []
    tasks: {}
    thread_id: 140184394814080
    transfer_incoming_log: []
    transfer_outgoing_log: []
    transition_counter: 0
    type: Worker
