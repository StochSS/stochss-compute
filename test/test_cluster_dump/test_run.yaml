scheduler:
  address: tcp://127.0.0.1:46479
  clients:
    fire-and-forget:
      client_key: fire-and-forget
      last_seen: 1675888189.069516
      wants_what: []
  events:
    all:
    - - 1675888189.093465
      - action: add-worker
        worker: tcp://127.0.0.1:41305
    - - 1675888189.0948532
      - action: add-worker
        worker: tcp://127.0.0.1:34761
    stealing: []
    tcp://127.0.0.1:34761:
    - - 1675888189.0948422
      - action: add-worker
    - - 1675888189.097894
      - action: worker-status-change
        prev-status: init
        status: running
    tcp://127.0.0.1:41305:
    - - 1675888189.093445
      - action: add-worker
    - - 1675888189.0977445
      - action: worker-status-change
        prev-status: init
        status: running
  extensions:
    amm: <distributed.active_memory_manager.ActiveMemoryManagerExtension object at
      0x7f7570c93450>
    events: <distributed.event.EventExtension object at 0x7f75716f5f50>
    locks: <distributed.lock.LockExtension object at 0x7f7570c85290>
    memory_sampler: <distributed.diagnostics.memory_sampler.MemorySamplerExtension
      object at 0x7f7570c93590>
    multi_locks: <distributed.multi_lock.MultiLockExtension object at 0x7f7570c92610>
    publish: <distributed.publish.PublishExtension object at 0x7f7570c926d0>
    pubsub: <distributed.pubsub.PubSubSchedulerExtension object at 0x7f7570c92d10>
    queues: <distributed.queues.QueueExtension object at 0x7f7570c92910>
    replay-tasks: <distributed.recreate_tasks.ReplayTaskScheduler object at 0x7f7570c92850>
    semaphores: <distributed.semaphore.SemaphoreExtension object at 0x7f7570c92e90>
    shuffle: <distributed.shuffle._scheduler_extension.ShuffleSchedulerExtension object
      at 0x7f7570c937d0>
    stealing:
      cost_multipliers:
      - 1.0
      - 1.03125
      - 1.0625
      - 1.125
      - 1.25
      - 1.5
      - 2
      - 3
      - 5
      - 9
      - 17
      - 33
      - 65
      - 129
      - 257
      count: 0
      in_flight: {}
      in_flight_occupancy: {}
      in_flight_tasks: {}
      key_stealable: {}
      metrics:
        request_cost_total: {}
        request_count_total: {}
      scheduler:
        address: tcp://127.0.0.1:46479
        clients:
          fire-and-forget: <Client 'fire-and-forget'>
        events:
          all:
          - - 1675888189.093465
            - action: add-worker
              worker: tcp://127.0.0.1:41305
          - - 1675888189.0948532
            - action: add-worker
              worker: tcp://127.0.0.1:34761
          stealing: []
          tcp://127.0.0.1:34761:
          - - 1675888189.0948422
            - action: add-worker
          - - 1675888189.097894
            - action: worker-status-change
              prev-status: init
              status: running
          tcp://127.0.0.1:41305:
          - - 1675888189.093445
            - action: add-worker
          - - 1675888189.0977445
            - action: worker-status-change
              prev-status: init
              status: running
        extensions: '{''locks'': <distributed.lock.LockExtension object at 0x7f7570c85290>,
          ''multi_locks'': <distributed.multi_lock.MultiLockExtension object at 0x7f7570c92610>,
          ''publish'': <distributed.publish.PublishExtension object at 0x7f7570c926d0>,
          ''replay-tasks'': <distributed.recreate_tasks.ReplayTaskScheduler object
          at 0x7f7570c92850>, ''queues'': <distributed.queues.QueueExtension object
          at 0x7f7570c92910>, ''variables'': <distributed.variable.VariableExtension
          object at 0x7f7570c92b50>, ''pubsub'': <distributed.pubsub.PubSubSchedulerExtension
          object at 0x7f7570c92d10>, ''semaphores'': <distributed.semaphore.SemaphoreExtension
          object at 0x7f7570c92e90>, ''events'': <distributed.event.EventExtension
          object at 0x7f75716f5f50>, ''amm'': <distributed.active_memory_manager.ActiveMemoryManagerExtension
          object at 0x7f7570c93450>, ''memory_sampler'': <distributed.diagnostics.memory_sampler.MemorySamplerExtension
          object at 0x7f7570c93590>, ''shuffle'': <distributed.shuffle._scheduler_extension.ShuffleSchedulerExtension
          object at 0x7f7570c937d0>, ''stealing'': <distributed.stealing.WorkStealing
          object at 0x7f75b386e810>}'
        id: Scheduler-a4290e7c-5b58-4c96-b75e-5109b8af5867
        memory:
          managed: 0
          managed_in_memory: 0
          managed_spilled: 0
          optimistic: 273317888
          process: 273317888
          unmanaged: 273317888
          unmanaged_old: 273317888
          unmanaged_recent: 0
        services:
          dashboard: 44215
        started: 1675888189.0515068
        status: running
        task_groups: {}
        tasks: {}
        thread_id: 140143508050560
        transition_counter: 0
        transition_log: []
        type: Scheduler
        workers:
          tcp://127.0.0.1:34761: '<WorkerState ''tcp://127.0.0.1:34761'', name: 1,
            status: running, memory: 0, processing: 0>'
          tcp://127.0.0.1:41305: '<WorkerState ''tcp://127.0.0.1:41305'', name: 0,
            status: running, memory: 0, processing: 0>'
      stealable:
        tcp://127.0.0.1:34761:
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        tcp://127.0.0.1:41305:
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
    variables: <distributed.variable.VariableExtension object at 0x7f7570c92b50>
  id: Scheduler-a4290e7c-5b58-4c96-b75e-5109b8af5867
  memory:
    managed: 0
    managed_in_memory: 0
    managed_spilled: 0
    optimistic: 273317888
    process: 273317888
    unmanaged: 273317888
    unmanaged_old: 273317888
    unmanaged_recent: 0
  services:
    dashboard: 44215
  started: 1675888189.0515068
  status: running
  task_groups: {}
  tasks: {}
  thread_id: 140143508050560
  transition_counter: 0
  transition_log: []
  type: Scheduler
  workers:
    tcp://127.0.0.1:34761:
      actors: []
      address: tcp://127.0.0.1:34761
      bandwidth: 100000000
      executing: {}
      extra: {}
      has_what: []
      host: 127.0.0.1
      last_seen: 1675888189.0948977
      local_directory: /tmp/dask-worker-space/worker-alejpr2w
      long_running: []
      memory:
        managed: 0
        managed_in_memory: 0
        managed_spilled: 0
        optimistic: 136658944
        process: 136658944
        unmanaged: 136658944
        unmanaged_old: 136658944
        unmanaged_recent: 0
      memory_limit: 16371576832
      metrics:
        bandwidth:
          total: 100000000
          types: {}
          workers: {}
        cpu: 0.0
        event_loop_interval: 0.5
        host_disk_io:
          read_bps: 0.0
          write_bps: 0.0
        host_net_io:
          read_bps: 0.0
          write_bps: 0.0
        managed_bytes: 0
        memory: 136658944
        num_fds: 15
        spilled_bytes:
          disk: 0
          memory: 0
        task_counts:
          constrained: 0
          executing: 0
          fetch: 0
          flight: 0
          long-running: 0
          memory: 0
          missing: 0
          other: 0
          ready: 0
          waiting: 0
        time: 1675888189.0814939
        transfer:
          incoming_bytes: 0
          incoming_count: 0
          incoming_count_total: 0
          outgoing_bytes: 0
          outgoing_count: 0
          outgoing_count_total: 0
      name: 1
      nanny: null
      nbytes: 0
      needs_what: {}
      nthreads: 2
      occupancy: 0.0
      pid: 310228
      processing: []
      resources: {}
      scheduler:
        address: tcp://127.0.0.1:46479
        clients:
          fire-and-forget: <Client 'fire-and-forget'>
        events:
          all:
          - - 1675888189.093465
            - action: add-worker
              worker: tcp://127.0.0.1:41305
          - - 1675888189.0948532
            - action: add-worker
              worker: tcp://127.0.0.1:34761
          stealing: []
          tcp://127.0.0.1:34761:
          - - 1675888189.0948422
            - action: add-worker
          - - 1675888189.097894
            - action: worker-status-change
              prev-status: init
              status: running
          tcp://127.0.0.1:41305:
          - - 1675888189.093445
            - action: add-worker
          - - 1675888189.0977445
            - action: worker-status-change
              prev-status: init
              status: running
        extensions:
          amm: <distributed.active_memory_manager.ActiveMemoryManagerExtension object
            at 0x7f7570c93450>
          events: <distributed.event.EventExtension object at 0x7f75716f5f50>
          locks: <distributed.lock.LockExtension object at 0x7f7570c85290>
          memory_sampler: <distributed.diagnostics.memory_sampler.MemorySamplerExtension
            object at 0x7f7570c93590>
          multi_locks: <distributed.multi_lock.MultiLockExtension object at 0x7f7570c92610>
          publish: <distributed.publish.PublishExtension object at 0x7f7570c926d0>
          pubsub: <distributed.pubsub.PubSubSchedulerExtension object at 0x7f7570c92d10>
          queues: <distributed.queues.QueueExtension object at 0x7f7570c92910>
          replay-tasks: <distributed.recreate_tasks.ReplayTaskScheduler object at
            0x7f7570c92850>
          semaphores: <distributed.semaphore.SemaphoreExtension object at 0x7f7570c92e90>
          shuffle: <distributed.shuffle._scheduler_extension.ShuffleSchedulerExtension
            object at 0x7f7570c937d0>
          stealing: <distributed.stealing.WorkStealing object at 0x7f75b386e810>
          variables: <distributed.variable.VariableExtension object at 0x7f7570c92b50>
        id: Scheduler-a4290e7c-5b58-4c96-b75e-5109b8af5867
        memory:
          managed: 0
          managed_in_memory: 0
          managed_spilled: 0
          optimistic: 273317888
          process: 273317888
          unmanaged: 273317888
          unmanaged_old: 273317888
          unmanaged_recent: 0
        services:
          dashboard: 44215
        started: 1675888189.0515068
        status: running
        task_groups: {}
        tasks: {}
        thread_id: 140143508050560
        transition_counter: 0
        transition_log: []
        type: Scheduler
        workers: 'SortedDict({''tcp://127.0.0.1:34761'': <WorkerState ''tcp://127.0.0.1:34761'',
          name: 1, status: running, memory: 0, processing: 0>, ''tcp://127.0.0.1:41305'':
          <WorkerState ''tcp://127.0.0.1:41305'', name: 0, status: running, memory:
          0, processing: 0>})'
      server_id: Worker-3b2f9c45-27ba-41af-9002-11d3b1b17ca3
      services:
        dashboard: 45795
      status: '<Status.running: ''running''>'
      task_prefix_count: {}
      time_delay: 0.003156900405883789
      used_resources: {}
    tcp://127.0.0.1:41305:
      actors: []
      address: tcp://127.0.0.1:41305
      bandwidth: 100000000
      executing: {}
      extra: {}
      has_what: []
      host: 127.0.0.1
      last_seen: 1675888189.0935314
      local_directory: /tmp/dask-worker-space/worker-ko08f9fu
      long_running: []
      memory:
        managed: 0
        managed_in_memory: 0
        managed_spilled: 0
        optimistic: 136658944
        process: 136658944
        unmanaged: 136658944
        unmanaged_old: 136658944
        unmanaged_recent: 0
      memory_limit: 16371576832
      metrics:
        bandwidth:
          total: 100000000
          types: {}
          workers: {}
        cpu: 0.0
        event_loop_interval: 0.5
        host_disk_io:
          read_bps: 0.0
          write_bps: 0.0
        host_net_io:
          read_bps: 0.0
          write_bps: 0.0
        managed_bytes: 0
        memory: 136658944
        num_fds: 14
        spilled_bytes:
          disk: 0
          memory: 0
        task_counts:
          constrained: 0
          executing: 0
          fetch: 0
          flight: 0
          long-running: 0
          memory: 0
          missing: 0
          other: 0
          ready: 0
          waiting: 0
        time: 1675888189.0775607
        transfer:
          incoming_bytes: 0
          incoming_count: 0
          incoming_count_total: 0
          outgoing_bytes: 0
          outgoing_count: 0
          outgoing_count_total: 0
      name: 0
      nanny: null
      nbytes: 0
      needs_what: {}
      nthreads: 1
      occupancy: 0.0
      pid: 310228
      processing: []
      resources: {}
      scheduler:
        address: tcp://127.0.0.1:46479
        clients:
          fire-and-forget: <Client 'fire-and-forget'>
        events:
          all:
          - - 1675888189.093465
            - action: add-worker
              worker: tcp://127.0.0.1:41305
          - - 1675888189.0948532
            - action: add-worker
              worker: tcp://127.0.0.1:34761
          stealing: []
          tcp://127.0.0.1:34761:
          - - 1675888189.0948422
            - action: add-worker
          - - 1675888189.097894
            - action: worker-status-change
              prev-status: init
              status: running
          tcp://127.0.0.1:41305:
          - - 1675888189.093445
            - action: add-worker
          - - 1675888189.0977445
            - action: worker-status-change
              prev-status: init
              status: running
        extensions:
          amm: <distributed.active_memory_manager.ActiveMemoryManagerExtension object
            at 0x7f7570c93450>
          events: <distributed.event.EventExtension object at 0x7f75716f5f50>
          locks: <distributed.lock.LockExtension object at 0x7f7570c85290>
          memory_sampler: <distributed.diagnostics.memory_sampler.MemorySamplerExtension
            object at 0x7f7570c93590>
          multi_locks: <distributed.multi_lock.MultiLockExtension object at 0x7f7570c92610>
          publish: <distributed.publish.PublishExtension object at 0x7f7570c926d0>
          pubsub: <distributed.pubsub.PubSubSchedulerExtension object at 0x7f7570c92d10>
          queues: <distributed.queues.QueueExtension object at 0x7f7570c92910>
          replay-tasks: <distributed.recreate_tasks.ReplayTaskScheduler object at
            0x7f7570c92850>
          semaphores: <distributed.semaphore.SemaphoreExtension object at 0x7f7570c92e90>
          shuffle: <distributed.shuffle._scheduler_extension.ShuffleSchedulerExtension
            object at 0x7f7570c937d0>
          stealing: <distributed.stealing.WorkStealing object at 0x7f75b386e810>
          variables: <distributed.variable.VariableExtension object at 0x7f7570c92b50>
        id: Scheduler-a4290e7c-5b58-4c96-b75e-5109b8af5867
        memory:
          managed: 0
          managed_in_memory: 0
          managed_spilled: 0
          optimistic: 273317888
          process: 273317888
          unmanaged: 273317888
          unmanaged_old: 273317888
          unmanaged_recent: 0
        services:
          dashboard: 44215
        started: 1675888189.0515068
        status: running
        task_groups: {}
        tasks: {}
        thread_id: 140143508050560
        transition_counter: 0
        transition_log: []
        type: Scheduler
        workers: 'SortedDict({''tcp://127.0.0.1:34761'': <WorkerState ''tcp://127.0.0.1:34761'',
          name: 1, status: running, memory: 0, processing: 0>, ''tcp://127.0.0.1:41305'':
          <WorkerState ''tcp://127.0.0.1:41305'', name: 0, status: running, memory:
          0, processing: 0>})'
      server_id: Worker-aba05934-57b4-48ab-8b97-1bb577215b52
      services:
        dashboard: 37503
      status: '<Status.running: ''running''>'
      task_prefix_count: {}
      time_delay: 0.004825592041015625
      used_resources: {}
versions:
  host:
    LANG: en_US.UTF-8
    LC_ALL: None
    OS: Linux
    OS-release: 5.15.0-58-generic
    byteorder: little
    machine: x86_64
    processor: x86_64
    python: 3.11.1.final.0
    python-bits: 64
  packages:
    cloudpickle: 2.2.0
    dask: 2022.12.1
    distributed: 2022.12.1
    lz4: null
    msgpack: 1.0.4
    numpy: 1.24.1
    pandas: null
    python: 3.11.1.final.0
    toolz: 0.12.0
    tornado: '6.2'
workers:
  tcp://127.0.0.1:34761:
    address: tcp://127.0.0.1:34761
    busy_workers: []
    config:
      array:
        backend: numpy
        chunk-size: 128MiB
        rechunk-threshold: 4
        slicing:
          split-large-chunks: null
        svg:
          size: 120
      dataframe:
        backend: pandas
        dtype_backend: pandas
        parquet:
          metadata-task-size-local: 512
          metadata-task-size-remote: 16
        shuffle-compression: null
      distributed:
        adaptive:
          interval: 1s
          maximum: .inf
          minimum: 0
          target-duration: 5s
          wait-count: 3
        admin:
          event-loop: tornado
          log-format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          log-length: 10000
          max-error-length: 10000
          pdb-on-err: false
          system-monitor:
            disk: true
            host-cpu: false
            interval: 500ms
          tick:
            cycle: 1s
            interval: 500 ms
            limit: 3s
        client:
          heartbeat: 5s
          preload: []
          preload-argv: []
          scheduler-info-interval: 2s
          security-loader: null
        comm:
          compression: auto
          default-scheme: tcp
          offload: 10MiB
          recent-messages-log-length: 0
          require-encryption: null
          retry:
            count: 0
            delay:
              max: 20s
              min: 1s
          shard: 64MiB
          socket-backlog: 2048
          tcp:
            backend: tornado
          timeouts:
            connect: 5s
            tcp: 30s
          tls:
            ca-file: null
            ciphers: null
            client:
              cert: null
              key: null
            max-version: null
            min-version: 1.2
            scheduler:
              cert: null
              key: null
            worker:
              cert: null
              key: null
          ucx:
            create-cuda-context: null
            cuda-copy: null
            environment: {}
            infiniband: null
            nvlink: null
            rdmacm: null
            tcp: null
          websockets:
            shard: 8MiB
          zstd:
            level: 3
            threads: 0
        dashboard:
          export-tool: false
          graph-max-items: 5000
          link: '{scheme}://{host}:{port}/status'
          prometheus:
            namespace: dask
        deploy:
          cluster-repr-interval: 500ms
          lost-worker-timeout: 15s
        diagnostics:
          computations:
            ignore-modules:
            - distributed
            - dask
            - xarray
            - cudf
            - cuml
            - prefect
            - xgboost
            max-history: 100
          erred-tasks:
            max-history: 100
          nvml: true
        nanny:
          environ: {}
          pre-spawn-environ:
            MALLOC_TRIM_THRESHOLD_: 65536
            MKL_NUM_THREADS: 1
            OMP_NUM_THREADS: 1
            OPENBLAS_NUM_THREADS: 1
          preload: []
          preload-argv: []
        rmm:
          pool-size: null
        scheduler:
          active-memory-manager:
            interval: 2s
            measure: optimistic
            policies:
            - class: distributed.active_memory_manager.ReduceReplicas
            start: true
          allowed-failures: 3
          allowed-imports:
          - dask
          - distributed
          bandwidth: 100000000
          blocked-handlers: []
          contact-address: null
          dashboard:
            bokeh-application:
              allow_websocket_origin:
              - '*'
              check_unused_sessions_milliseconds: 500
              keep_alive_milliseconds: 500
            status:
              task-stream-length: 1000
            tasks:
              task-stream-length: 100000
            tls:
              ca-file: null
              cert: null
              key: null
          default-data-size: 1kiB
          default-task-durations:
            rechunk-split: 1us
            split-shuffle: 1us
          events-cleanup-delay: 1h
          events-log-length: 100000
          http:
            routes:
            - distributed.http.scheduler.prometheus
            - distributed.http.scheduler.info
            - distributed.http.scheduler.json
            - distributed.http.health
            - distributed.http.proxy
            - distributed.http.statics
          idle-timeout: null
          locks:
            lease-timeout: 30s
            lease-validation-interval: 10s
          pickle: true
          preload: []
          preload-argv: []
          transition-log-length: 100000
          unknown-task-duration: 500ms
          validate: false
          work-stealing: true
          work-stealing-interval: 100ms
          worker-saturation: 1.1
          worker-ttl: 5 minutes
        version: 2
        worker:
          blocked-handlers: []
          connections:
            incoming: 10
            outgoing: 50
          daemon: true
          http:
            routes:
            - distributed.http.worker.prometheus
            - distributed.http.health
            - distributed.http.statics
          lifetime:
            duration: null
            restart: false
            stagger: 0 seconds
          memory:
            max-spill: false
            monitor-interval: 100ms
            pause: 0.8
            rebalance:
              measure: optimistic
              recipient-max: 0.6
              sender-min: 0.3
              sender-recipient-gap: 0.1
            recent-to-old-time: 30s
            spill: 0.7
            target: 0.6
            terminate: 0.95
            transfer: 0.1
          multiprocessing-method: spawn
          preload: []
          preload-argv: []
          profile:
            cycle: 1000ms
            enabled: false
            interval: 10ms
            low-level: false
          resources: {}
          transfer:
            message-bytes-limit: 50MB
          use-file-locking: true
          validate: false
      local_directory: /tmp
      optimization:
        annotations:
          fuse: true
        fuse:
          active: null
          ave-width: 1
          max-depth-new-edges: null
          max-height: .inf
          max-width: null
          rename-keys: true
          subgraphs: null
      temporary-directory: null
      tokenize:
        ensure-deterministic: false
      visualization:
        engine: null
    constrained: []
    data: {}
    data_needed: {}
    executing: []
    has_what: {}
    id: Worker-3b2f9c45-27ba-41af-9002-11d3b1b17ca3
    in_flight_tasks: []
    in_flight_workers: {}
    log: []
    logs:
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -       Start worker
        at:      tcp://127.0.0.1:41305'
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -          Listening
        to:      tcp://127.0.0.1:41305'
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -           Worker name:                          0'
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -          dashboard
        at:            127.0.0.1:37503'
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO - Waiting to connect
        to:      tcp://127.0.0.1:46479'
    - - INFO
      - 2023-02-08 15:29:49,084 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -               Threads:                          1'
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -                Memory:                  15.25
        GiB'
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -       Local Directory:
        /tmp/dask-worker-space/worker-ko08f9fu'
    - - INFO
      - 2023-02-08 15:29:49,085 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-02-08 15:29:49,085 - distributed.worker - INFO -       Start worker
        at:      tcp://127.0.0.1:34761'
    - - INFO
      - '2023-02-08 15:29:49,085 - distributed.worker - INFO -          Listening
        to:      tcp://127.0.0.1:34761'
    - - INFO
      - '2023-02-08 15:29:49,085 - distributed.worker - INFO -           Worker name:                          1'
    - - INFO
      - '2023-02-08 15:29:49,085 - distributed.worker - INFO -          dashboard
        at:            127.0.0.1:45795'
    - - INFO
      - '2023-02-08 15:29:49,085 - distributed.worker - INFO - Waiting to connect
        to:      tcp://127.0.0.1:46479'
    - - INFO
      - 2023-02-08 15:29:49,085 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-02-08 15:29:49,085 - distributed.worker - INFO -               Threads:                          2'
    - - INFO
      - '2023-02-08 15:29:49,086 - distributed.worker - INFO -                Memory:                  15.25
        GiB'
    - - INFO
      - '2023-02-08 15:29:49,086 - distributed.worker - INFO -       Local Directory:
        /tmp/dask-worker-space/worker-alejpr2w'
    - - INFO
      - 2023-02-08 15:29:49,086 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-02-08 15:29:49,096 - distributed.worker - INFO -         Registered
        to:      tcp://127.0.0.1:46479'
    - - INFO
      - 2023-02-08 15:29:49,096 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-02-08 15:29:49,096 - distributed.worker - INFO -         Registered
        to:      tcp://127.0.0.1:46479'
    - - INFO
      - 2023-02-08 15:29:49,096 - distributed.worker - INFO - -------------------------------------------------
    long_running: []
    max_spill: false
    memory_limit: 16371576832
    memory_monitor_interval: 0.1
    memory_pause_fraction: 0.8
    memory_spill_fraction: 0.7
    memory_target_fraction: 0.6
    missing_dep_flight: []
    nthreads: 2
    ready: []
    running: true
    scheduler: tcp://127.0.0.1:46479
    status: '<Status.running: ''running''>'
    stimulus_log: []
    tasks: {}
    thread_id: 140143508050560
    transfer_incoming_log: []
    transfer_outgoing_log: []
    transition_counter: 0
    type: Worker
  tcp://127.0.0.1:41305:
    address: tcp://127.0.0.1:41305
    busy_workers: []
    config:
      array:
        backend: numpy
        chunk-size: 128MiB
        rechunk-threshold: 4
        slicing:
          split-large-chunks: null
        svg:
          size: 120
      dataframe:
        backend: pandas
        dtype_backend: pandas
        parquet:
          metadata-task-size-local: 512
          metadata-task-size-remote: 16
        shuffle-compression: null
      distributed:
        adaptive:
          interval: 1s
          maximum: .inf
          minimum: 0
          target-duration: 5s
          wait-count: 3
        admin:
          event-loop: tornado
          log-format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          log-length: 10000
          max-error-length: 10000
          pdb-on-err: false
          system-monitor:
            disk: true
            host-cpu: false
            interval: 500ms
          tick:
            cycle: 1s
            interval: 500 ms
            limit: 3s
        client:
          heartbeat: 5s
          preload: []
          preload-argv: []
          scheduler-info-interval: 2s
          security-loader: null
        comm:
          compression: auto
          default-scheme: tcp
          offload: 10MiB
          recent-messages-log-length: 0
          require-encryption: null
          retry:
            count: 0
            delay:
              max: 20s
              min: 1s
          shard: 64MiB
          socket-backlog: 2048
          tcp:
            backend: tornado
          timeouts:
            connect: 5s
            tcp: 30s
          tls:
            ca-file: null
            ciphers: null
            client:
              cert: null
              key: null
            max-version: null
            min-version: 1.2
            scheduler:
              cert: null
              key: null
            worker:
              cert: null
              key: null
          ucx:
            create-cuda-context: null
            cuda-copy: null
            environment: {}
            infiniband: null
            nvlink: null
            rdmacm: null
            tcp: null
          websockets:
            shard: 8MiB
          zstd:
            level: 3
            threads: 0
        dashboard:
          export-tool: false
          graph-max-items: 5000
          link: '{scheme}://{host}:{port}/status'
          prometheus:
            namespace: dask
        deploy:
          cluster-repr-interval: 500ms
          lost-worker-timeout: 15s
        diagnostics:
          computations:
            ignore-modules:
            - distributed
            - dask
            - xarray
            - cudf
            - cuml
            - prefect
            - xgboost
            max-history: 100
          erred-tasks:
            max-history: 100
          nvml: true
        nanny:
          environ: {}
          pre-spawn-environ:
            MALLOC_TRIM_THRESHOLD_: 65536
            MKL_NUM_THREADS: 1
            OMP_NUM_THREADS: 1
            OPENBLAS_NUM_THREADS: 1
          preload: []
          preload-argv: []
        rmm:
          pool-size: null
        scheduler:
          active-memory-manager:
            interval: 2s
            measure: optimistic
            policies:
            - class: distributed.active_memory_manager.ReduceReplicas
            start: true
          allowed-failures: 3
          allowed-imports:
          - dask
          - distributed
          bandwidth: 100000000
          blocked-handlers: []
          contact-address: null
          dashboard:
            bokeh-application:
              allow_websocket_origin:
              - '*'
              check_unused_sessions_milliseconds: 500
              keep_alive_milliseconds: 500
            status:
              task-stream-length: 1000
            tasks:
              task-stream-length: 100000
            tls:
              ca-file: null
              cert: null
              key: null
          default-data-size: 1kiB
          default-task-durations:
            rechunk-split: 1us
            split-shuffle: 1us
          events-cleanup-delay: 1h
          events-log-length: 100000
          http:
            routes:
            - distributed.http.scheduler.prometheus
            - distributed.http.scheduler.info
            - distributed.http.scheduler.json
            - distributed.http.health
            - distributed.http.proxy
            - distributed.http.statics
          idle-timeout: null
          locks:
            lease-timeout: 30s
            lease-validation-interval: 10s
          pickle: true
          preload: []
          preload-argv: []
          transition-log-length: 100000
          unknown-task-duration: 500ms
          validate: false
          work-stealing: true
          work-stealing-interval: 100ms
          worker-saturation: 1.1
          worker-ttl: 5 minutes
        version: 2
        worker:
          blocked-handlers: []
          connections:
            incoming: 10
            outgoing: 50
          daemon: true
          http:
            routes:
            - distributed.http.worker.prometheus
            - distributed.http.health
            - distributed.http.statics
          lifetime:
            duration: null
            restart: false
            stagger: 0 seconds
          memory:
            max-spill: false
            monitor-interval: 100ms
            pause: 0.8
            rebalance:
              measure: optimistic
              recipient-max: 0.6
              sender-min: 0.3
              sender-recipient-gap: 0.1
            recent-to-old-time: 30s
            spill: 0.7
            target: 0.6
            terminate: 0.95
            transfer: 0.1
          multiprocessing-method: spawn
          preload: []
          preload-argv: []
          profile:
            cycle: 1000ms
            enabled: false
            interval: 10ms
            low-level: false
          resources: {}
          transfer:
            message-bytes-limit: 50MB
          use-file-locking: true
          validate: false
      local_directory: /tmp
      optimization:
        annotations:
          fuse: true
        fuse:
          active: null
          ave-width: 1
          max-depth-new-edges: null
          max-height: .inf
          max-width: null
          rename-keys: true
          subgraphs: null
      temporary-directory: null
      tokenize:
        ensure-deterministic: false
      visualization:
        engine: null
    constrained: []
    data: {}
    data_needed: {}
    executing: []
    has_what: {}
    id: Worker-aba05934-57b4-48ab-8b97-1bb577215b52
    in_flight_tasks: []
    in_flight_workers: {}
    log: []
    logs:
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -       Start worker
        at:      tcp://127.0.0.1:41305'
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -          Listening
        to:      tcp://127.0.0.1:41305'
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -           Worker name:                          0'
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -          dashboard
        at:            127.0.0.1:37503'
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO - Waiting to connect
        to:      tcp://127.0.0.1:46479'
    - - INFO
      - 2023-02-08 15:29:49,084 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -               Threads:                          1'
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -                Memory:                  15.25
        GiB'
    - - INFO
      - '2023-02-08 15:29:49,084 - distributed.worker - INFO -       Local Directory:
        /tmp/dask-worker-space/worker-ko08f9fu'
    - - INFO
      - 2023-02-08 15:29:49,085 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-02-08 15:29:49,085 - distributed.worker - INFO -       Start worker
        at:      tcp://127.0.0.1:34761'
    - - INFO
      - '2023-02-08 15:29:49,085 - distributed.worker - INFO -          Listening
        to:      tcp://127.0.0.1:34761'
    - - INFO
      - '2023-02-08 15:29:49,085 - distributed.worker - INFO -           Worker name:                          1'
    - - INFO
      - '2023-02-08 15:29:49,085 - distributed.worker - INFO -          dashboard
        at:            127.0.0.1:45795'
    - - INFO
      - '2023-02-08 15:29:49,085 - distributed.worker - INFO - Waiting to connect
        to:      tcp://127.0.0.1:46479'
    - - INFO
      - 2023-02-08 15:29:49,085 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-02-08 15:29:49,085 - distributed.worker - INFO -               Threads:                          2'
    - - INFO
      - '2023-02-08 15:29:49,086 - distributed.worker - INFO -                Memory:                  15.25
        GiB'
    - - INFO
      - '2023-02-08 15:29:49,086 - distributed.worker - INFO -       Local Directory:
        /tmp/dask-worker-space/worker-alejpr2w'
    - - INFO
      - 2023-02-08 15:29:49,086 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-02-08 15:29:49,096 - distributed.worker - INFO -         Registered
        to:      tcp://127.0.0.1:46479'
    - - INFO
      - 2023-02-08 15:29:49,096 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-02-08 15:29:49,096 - distributed.worker - INFO -         Registered
        to:      tcp://127.0.0.1:46479'
    - - INFO
      - 2023-02-08 15:29:49,096 - distributed.worker - INFO - -------------------------------------------------
    long_running: []
    max_spill: false
    memory_limit: 16371576832
    memory_monitor_interval: 0.1
    memory_pause_fraction: 0.8
    memory_spill_fraction: 0.7
    memory_target_fraction: 0.6
    missing_dep_flight: []
    nthreads: 1
    ready: []
    running: true
    scheduler: tcp://127.0.0.1:46479
    status: '<Status.running: ''running''>'
    stimulus_log: []
    tasks: {}
    thread_id: 140143508050560
    transfer_incoming_log: []
    transfer_outgoing_log: []
    transition_counter: 0
    type: Worker
