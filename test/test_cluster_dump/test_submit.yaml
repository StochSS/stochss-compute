scheduler:
  address: tcp://127.0.0.1:35679
  clients:
    Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227:
      client_key: Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227
      last_seen: 1674596971.9424255
      wants_what: []
    fire-and-forget:
      client_key: fire-and-forget
      last_seen: 1674596971.9029233
      wants_what: []
  events:
    Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227:
    - - 1674596971.9424117
      - action: add-client
        client: Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227
    all:
    - - 1674596971.9233587
      - action: add-worker
        worker: tcp://127.0.0.1:34489
    - - 1674596971.9246614
      - action: add-worker
        worker: tcp://127.0.0.1:46283
    - - 1674596971.9424117
      - action: add-client
        client: Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227
    stealing: []
    tcp://127.0.0.1:34489:
    - - 1674596971.9233422
      - action: add-worker
    - - 1674596971.9275804
      - action: worker-status-change
        prev-status: init
        status: running
    tcp://127.0.0.1:46283:
    - - 1674596971.9246507
      - action: add-worker
    - - 1674596971.927832
      - action: worker-status-change
        prev-status: init
        status: running
  extensions:
    amm: <distributed.active_memory_manager.ActiveMemoryManagerExtension object at
      0x7f3834f24d50>
    events: <distributed.event.EventExtension object at 0x7f3834f24cd0>
    locks: <distributed.lock.LockExtension object at 0x7f3834efee50>
    memory_sampler: <distributed.diagnostics.memory_sampler.MemorySamplerExtension
      object at 0x7f3834f24e90>
    multi_locks: <distributed.multi_lock.MultiLockExtension object at 0x7f3834efec10>
    publish: <distributed.publish.PublishExtension object at 0x7f3837a7d790>
    pubsub: <distributed.pubsub.PubSubSchedulerExtension object at 0x7f3834f24610>
    queues: <distributed.queues.QueueExtension object at 0x7f3834f24110>
    replay-tasks: <distributed.recreate_tasks.ReplayTaskScheduler object at 0x7f3834f24050>
    semaphores: <distributed.semaphore.SemaphoreExtension object at 0x7f3834f24790>
    shuffle: <distributed.shuffle._scheduler_extension.ShuffleSchedulerExtension object
      at 0x7f3834f250d0>
    stealing:
      cost_multipliers:
      - 1.0
      - 1.03125
      - 1.0625
      - 1.125
      - 1.25
      - 1.5
      - 2
      - 3
      - 5
      - 9
      - 17
      - 33
      - 65
      - 129
      - 257
      count: 0
      in_flight: {}
      in_flight_occupancy: {}
      in_flight_tasks: {}
      key_stealable: {}
      metrics:
        request_cost_total: {}
        request_count_total: {}
      scheduler:
        address: tcp://127.0.0.1:35679
        clients:
          Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227: <Client 'Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227'>
          fire-and-forget: <Client 'fire-and-forget'>
        events:
          Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227:
          - - 1674596971.9424117
            - action: add-client
              client: Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227
          all:
          - - 1674596971.9233587
            - action: add-worker
              worker: tcp://127.0.0.1:34489
          - - 1674596971.9246614
            - action: add-worker
              worker: tcp://127.0.0.1:46283
          - - 1674596971.9424117
            - action: add-client
              client: Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227
          stealing: []
          tcp://127.0.0.1:34489:
          - - 1674596971.9233422
            - action: add-worker
          - - 1674596971.9275804
            - action: worker-status-change
              prev-status: init
              status: running
          tcp://127.0.0.1:46283:
          - - 1674596971.9246507
            - action: add-worker
          - - 1674596971.927832
            - action: worker-status-change
              prev-status: init
              status: running
        extensions: '{''locks'': <distributed.lock.LockExtension object at 0x7f3834efee50>,
          ''multi_locks'': <distributed.multi_lock.MultiLockExtension object at 0x7f3834efec10>,
          ''publish'': <distributed.publish.PublishExtension object at 0x7f3837a7d790>,
          ''replay-tasks'': <distributed.recreate_tasks.ReplayTaskScheduler object
          at 0x7f3834f24050>, ''queues'': <distributed.queues.QueueExtension object
          at 0x7f3834f24110>, ''variables'': <distributed.variable.VariableExtension
          object at 0x7f3834f24350>, ''pubsub'': <distributed.pubsub.PubSubSchedulerExtension
          object at 0x7f3834f24610>, ''semaphores'': <distributed.semaphore.SemaphoreExtension
          object at 0x7f3834f24790>, ''events'': <distributed.event.EventExtension
          object at 0x7f3834f24cd0>, ''amm'': <distributed.active_memory_manager.ActiveMemoryManagerExtension
          object at 0x7f3834f24d50>, ''memory_sampler'': <distributed.diagnostics.memory_sampler.MemorySamplerExtension
          object at 0x7f3834f24e90>, ''shuffle'': <distributed.shuffle._scheduler_extension.ShuffleSchedulerExtension
          object at 0x7f3834f250d0>, ''stealing'': <distributed.stealing.WorkStealing
          object at 0x7f38359e4790>}'
        id: Scheduler-7057ff0a-c326-4625-9e3f-f1c8ce9822b1
        memory:
          managed: 0
          managed_in_memory: 0
          managed_spilled: 0
          optimistic: 271237120
          process: 271237120
          unmanaged: 271237120
          unmanaged_old: 271237120
          unmanaged_recent: 0
        services:
          dashboard: 34611
        started: 1674596971.8834655
        status: running
        task_groups: {}
        tasks: {}
        thread_id: 139880511607424
        transition_counter: 0
        transition_log: []
        type: Scheduler
        workers:
          tcp://127.0.0.1:34489: '<WorkerState ''tcp://127.0.0.1:34489'', name: 0,
            status: running, memory: 0, processing: 0>'
          tcp://127.0.0.1:46283: '<WorkerState ''tcp://127.0.0.1:46283'', name: 1,
            status: running, memory: 0, processing: 0>'
      stealable:
        tcp://127.0.0.1:34489:
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        tcp://127.0.0.1:46283:
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
    variables: <distributed.variable.VariableExtension object at 0x7f3834f24350>
  id: Scheduler-7057ff0a-c326-4625-9e3f-f1c8ce9822b1
  memory:
    managed: 0
    managed_in_memory: 0
    managed_spilled: 0
    optimistic: 271237120
    process: 271237120
    unmanaged: 271237120
    unmanaged_old: 271237120
    unmanaged_recent: 0
  services:
    dashboard: 34611
  started: 1674596971.8834655
  status: running
  task_groups: {}
  tasks: {}
  thread_id: 139880511607424
  transition_counter: 0
  transition_log: []
  type: Scheduler
  workers:
    tcp://127.0.0.1:34489:
      actors: []
      address: tcp://127.0.0.1:34489
      bandwidth: 100000000
      executing: {}
      extra: {}
      has_what: []
      host: 127.0.0.1
      last_seen: 1674596971.9234245
      local_directory: /tmp/dask-worker-space/worker-qiaf9shy
      long_running: []
      memory:
        managed: 0
        managed_in_memory: 0
        managed_spilled: 0
        optimistic: 135483392
        process: 135483392
        unmanaged: 135483392
        unmanaged_old: 135483392
        unmanaged_recent: 0
      memory_limit: 16371576832
      metrics:
        bandwidth:
          total: 100000000
          types: {}
          workers: {}
        cpu: 0.0
        event_loop_interval: 0.5
        host_disk_io:
          read_bps: 0.0
          write_bps: 0.0
        host_net_io:
          read_bps: 0.0
          write_bps: 0.0
        managed_bytes: 0
        memory: 135483392
        num_fds: 18
        spilled_bytes:
          disk: 0
          memory: 0
        task_counts:
          constrained: 0
          executing: 0
          fetch: 0
          flight: 0
          long-running: 0
          memory: 0
          missing: 0
          other: 0
          ready: 0
          waiting: 0
        time: 1674596971.908842
        transfer:
          incoming_bytes: 0
          incoming_count: 0
          incoming_count_total: 0
          outgoing_bytes: 0
          outgoing_count: 0
          outgoing_count_total: 0
      name: 0
      nanny: null
      nbytes: 0
      needs_what: {}
      nthreads: 1
      occupancy: 0.0
      pid: 355852
      processing: []
      resources: {}
      scheduler:
        address: tcp://127.0.0.1:35679
        clients:
          Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227: <Client 'Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227'>
          fire-and-forget: <Client 'fire-and-forget'>
        events:
          Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227:
          - - 1674596971.9424117
            - action: add-client
              client: Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227
          all:
          - - 1674596971.9233587
            - action: add-worker
              worker: tcp://127.0.0.1:34489
          - - 1674596971.9246614
            - action: add-worker
              worker: tcp://127.0.0.1:46283
          - - 1674596971.9424117
            - action: add-client
              client: Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227
          stealing: []
          tcp://127.0.0.1:34489:
          - - 1674596971.9233422
            - action: add-worker
          - - 1674596971.9275804
            - action: worker-status-change
              prev-status: init
              status: running
          tcp://127.0.0.1:46283:
          - - 1674596971.9246507
            - action: add-worker
          - - 1674596971.927832
            - action: worker-status-change
              prev-status: init
              status: running
        extensions:
          amm: <distributed.active_memory_manager.ActiveMemoryManagerExtension object
            at 0x7f3834f24d50>
          events: <distributed.event.EventExtension object at 0x7f3834f24cd0>
          locks: <distributed.lock.LockExtension object at 0x7f3834efee50>
          memory_sampler: <distributed.diagnostics.memory_sampler.MemorySamplerExtension
            object at 0x7f3834f24e90>
          multi_locks: <distributed.multi_lock.MultiLockExtension object at 0x7f3834efec10>
          publish: <distributed.publish.PublishExtension object at 0x7f3837a7d790>
          pubsub: <distributed.pubsub.PubSubSchedulerExtension object at 0x7f3834f24610>
          queues: <distributed.queues.QueueExtension object at 0x7f3834f24110>
          replay-tasks: <distributed.recreate_tasks.ReplayTaskScheduler object at
            0x7f3834f24050>
          semaphores: <distributed.semaphore.SemaphoreExtension object at 0x7f3834f24790>
          shuffle: <distributed.shuffle._scheduler_extension.ShuffleSchedulerExtension
            object at 0x7f3834f250d0>
          stealing: <distributed.stealing.WorkStealing object at 0x7f38359e4790>
          variables: <distributed.variable.VariableExtension object at 0x7f3834f24350>
        id: Scheduler-7057ff0a-c326-4625-9e3f-f1c8ce9822b1
        memory:
          managed: 0
          managed_in_memory: 0
          managed_spilled: 0
          optimistic: 271237120
          process: 271237120
          unmanaged: 271237120
          unmanaged_old: 271237120
          unmanaged_recent: 0
        services:
          dashboard: 34611
        started: 1674596971.8834655
        status: running
        task_groups: {}
        tasks: {}
        thread_id: 139880511607424
        transition_counter: 0
        transition_log: []
        type: Scheduler
        workers: 'SortedDict({''tcp://127.0.0.1:34489'': <WorkerState ''tcp://127.0.0.1:34489'',
          name: 0, status: running, memory: 0, processing: 0>, ''tcp://127.0.0.1:46283'':
          <WorkerState ''tcp://127.0.0.1:46283'', name: 1, status: running, memory:
          0, processing: 0>})'
      server_id: Worker-c85b8f71-7ad0-4e08-bbfa-0f92dd6025af
      services:
        dashboard: 43333
      status: '<Status.running: ''running''>'
      task_prefix_count: {}
      time_delay: 0.004243135452270508
      used_resources: {}
    tcp://127.0.0.1:46283:
      actors: []
      address: tcp://127.0.0.1:46283
      bandwidth: 100000000
      executing: {}
      extra: {}
      has_what: []
      host: 127.0.0.1
      last_seen: 1674596971.9247026
      local_directory: /tmp/dask-worker-space/worker-44azq52g
      long_running: []
      memory:
        managed: 0
        managed_in_memory: 0
        managed_spilled: 0
        optimistic: 135753728
        process: 135753728
        unmanaged: 135753728
        unmanaged_old: 135753728
        unmanaged_recent: 0
      memory_limit: 16371576832
      metrics:
        bandwidth:
          total: 100000000
          types: {}
          workers: {}
        cpu: 0.0
        event_loop_interval: 0.5
        host_disk_io:
          read_bps: 0.0
          write_bps: 0.0
        host_net_io:
          read_bps: 0.0
          write_bps: 0.0
        managed_bytes: 0
        memory: 135753728
        num_fds: 19
        spilled_bytes:
          disk: 0
          memory: 0
        task_counts:
          constrained: 0
          executing: 0
          fetch: 0
          flight: 0
          long-running: 0
          memory: 0
          missing: 0
          other: 0
          ready: 0
          waiting: 0
        time: 1674596971.9118414
        transfer:
          incoming_bytes: 0
          incoming_count: 0
          incoming_count_total: 0
          outgoing_bytes: 0
          outgoing_count: 0
          outgoing_count_total: 0
      name: 1
      nanny: null
      nbytes: 0
      needs_what: {}
      nthreads: 2
      occupancy: 0.0
      pid: 355852
      processing: []
      resources: {}
      scheduler:
        address: tcp://127.0.0.1:35679
        clients:
          Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227: <Client 'Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227'>
          fire-and-forget: <Client 'fire-and-forget'>
        events:
          Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227:
          - - 1674596971.9424117
            - action: add-client
              client: Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227
          all:
          - - 1674596971.9233587
            - action: add-worker
              worker: tcp://127.0.0.1:34489
          - - 1674596971.9246614
            - action: add-worker
              worker: tcp://127.0.0.1:46283
          - - 1674596971.9424117
            - action: add-client
              client: Client-fc0c34b2-9c30-11ed-ae0c-f3c219d0b227
          stealing: []
          tcp://127.0.0.1:34489:
          - - 1674596971.9233422
            - action: add-worker
          - - 1674596971.9275804
            - action: worker-status-change
              prev-status: init
              status: running
          tcp://127.0.0.1:46283:
          - - 1674596971.9246507
            - action: add-worker
          - - 1674596971.927832
            - action: worker-status-change
              prev-status: init
              status: running
        extensions:
          amm: <distributed.active_memory_manager.ActiveMemoryManagerExtension object
            at 0x7f3834f24d50>
          events: <distributed.event.EventExtension object at 0x7f3834f24cd0>
          locks: <distributed.lock.LockExtension object at 0x7f3834efee50>
          memory_sampler: <distributed.diagnostics.memory_sampler.MemorySamplerExtension
            object at 0x7f3834f24e90>
          multi_locks: <distributed.multi_lock.MultiLockExtension object at 0x7f3834efec10>
          publish: <distributed.publish.PublishExtension object at 0x7f3837a7d790>
          pubsub: <distributed.pubsub.PubSubSchedulerExtension object at 0x7f3834f24610>
          queues: <distributed.queues.QueueExtension object at 0x7f3834f24110>
          replay-tasks: <distributed.recreate_tasks.ReplayTaskScheduler object at
            0x7f3834f24050>
          semaphores: <distributed.semaphore.SemaphoreExtension object at 0x7f3834f24790>
          shuffle: <distributed.shuffle._scheduler_extension.ShuffleSchedulerExtension
            object at 0x7f3834f250d0>
          stealing: <distributed.stealing.WorkStealing object at 0x7f38359e4790>
          variables: <distributed.variable.VariableExtension object at 0x7f3834f24350>
        id: Scheduler-7057ff0a-c326-4625-9e3f-f1c8ce9822b1
        memory:
          managed: 0
          managed_in_memory: 0
          managed_spilled: 0
          optimistic: 271237120
          process: 271237120
          unmanaged: 271237120
          unmanaged_old: 271237120
          unmanaged_recent: 0
        services:
          dashboard: 34611
        started: 1674596971.8834655
        status: running
        task_groups: {}
        tasks: {}
        thread_id: 139880511607424
        transition_counter: 0
        transition_log: []
        type: Scheduler
        workers: 'SortedDict({''tcp://127.0.0.1:34489'': <WorkerState ''tcp://127.0.0.1:34489'',
          name: 0, status: running, memory: 0, processing: 0>, ''tcp://127.0.0.1:46283'':
          <WorkerState ''tcp://127.0.0.1:46283'', name: 1, status: running, memory:
          0, processing: 0>})'
      server_id: Worker-611ccaad-dacf-4f79-964f-e107a85b2406
      services:
        dashboard: 46415
      status: '<Status.running: ''running''>'
      task_prefix_count: {}
      time_delay: 0.0027103424072265625
      used_resources: {}
versions:
  host:
    LANG: en_US.UTF-8
    LC_ALL: None
    OS: Linux
    OS-release: 5.15.0-58-generic
    byteorder: little
    machine: x86_64
    processor: x86_64
    python: 3.11.1.final.0
    python-bits: 64
  packages:
    cloudpickle: 2.2.0
    dask: 2022.12.1
    distributed: 2022.12.1
    lz4: null
    msgpack: 1.0.4
    numpy: 1.24.1
    pandas: null
    python: 3.11.1.final.0
    toolz: 0.12.0
    tornado: '6.2'
workers:
  tcp://127.0.0.1:34489:
    address: tcp://127.0.0.1:34489
    busy_workers: []
    config:
      array:
        backend: numpy
        chunk-size: 128MiB
        rechunk-threshold: 4
        slicing:
          split-large-chunks: null
        svg:
          size: 120
      dataframe:
        backend: pandas
        dtype_backend: pandas
        parquet:
          metadata-task-size-local: 512
          metadata-task-size-remote: 16
        shuffle-compression: null
      distributed:
        adaptive:
          interval: 1s
          maximum: .inf
          minimum: 0
          target-duration: 5s
          wait-count: 3
        admin:
          event-loop: tornado
          log-format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          log-length: 10000
          max-error-length: 10000
          pdb-on-err: false
          system-monitor:
            disk: true
            host-cpu: false
            interval: 500ms
          tick:
            cycle: 1s
            interval: 500 ms
            limit: 3s
        client:
          heartbeat: 5s
          preload: []
          preload-argv: []
          scheduler-info-interval: 2s
          security-loader: null
        comm:
          compression: auto
          default-scheme: tcp
          offload: 10MiB
          recent-messages-log-length: 0
          require-encryption: null
          retry:
            count: 0
            delay:
              max: 20s
              min: 1s
          shard: 64MiB
          socket-backlog: 2048
          tcp:
            backend: tornado
          timeouts:
            connect: 5s
            tcp: 30s
          tls:
            ca-file: null
            ciphers: null
            client:
              cert: null
              key: null
            max-version: null
            min-version: 1.2
            scheduler:
              cert: null
              key: null
            worker:
              cert: null
              key: null
          ucx:
            create-cuda-context: null
            cuda-copy: null
            environment: {}
            infiniband: null
            nvlink: null
            rdmacm: null
            tcp: null
          websockets:
            shard: 8MiB
          zstd:
            level: 3
            threads: 0
        dashboard:
          export-tool: false
          graph-max-items: 5000
          link: '{scheme}://{host}:{port}/status'
          prometheus:
            namespace: dask
        deploy:
          cluster-repr-interval: 500ms
          lost-worker-timeout: 15s
        diagnostics:
          computations:
            ignore-modules:
            - distributed
            - dask
            - xarray
            - cudf
            - cuml
            - prefect
            - xgboost
            max-history: 100
          erred-tasks:
            max-history: 100
          nvml: true
        nanny:
          environ: {}
          pre-spawn-environ:
            MALLOC_TRIM_THRESHOLD_: 65536
            MKL_NUM_THREADS: 1
            OMP_NUM_THREADS: 1
            OPENBLAS_NUM_THREADS: 1
          preload: []
          preload-argv: []
        rmm:
          pool-size: null
        scheduler:
          active-memory-manager:
            interval: 2s
            measure: optimistic
            policies:
            - class: distributed.active_memory_manager.ReduceReplicas
            start: true
          allowed-failures: 3
          allowed-imports:
          - dask
          - distributed
          bandwidth: 100000000
          blocked-handlers: []
          contact-address: null
          dashboard:
            bokeh-application:
              allow_websocket_origin:
              - '*'
              check_unused_sessions_milliseconds: 500
              keep_alive_milliseconds: 500
            status:
              task-stream-length: 1000
            tasks:
              task-stream-length: 100000
            tls:
              ca-file: null
              cert: null
              key: null
          default-data-size: 1kiB
          default-task-durations:
            rechunk-split: 1us
            split-shuffle: 1us
          events-cleanup-delay: 1h
          events-log-length: 100000
          http:
            routes:
            - distributed.http.scheduler.prometheus
            - distributed.http.scheduler.info
            - distributed.http.scheduler.json
            - distributed.http.health
            - distributed.http.proxy
            - distributed.http.statics
          idle-timeout: null
          locks:
            lease-timeout: 30s
            lease-validation-interval: 10s
          pickle: true
          preload: []
          preload-argv: []
          transition-log-length: 100000
          unknown-task-duration: 500ms
          validate: false
          work-stealing: true
          work-stealing-interval: 100ms
          worker-saturation: 1.1
          worker-ttl: 5 minutes
        version: 2
        worker:
          blocked-handlers: []
          connections:
            incoming: 10
            outgoing: 50
          daemon: true
          http:
            routes:
            - distributed.http.worker.prometheus
            - distributed.http.health
            - distributed.http.statics
          lifetime:
            duration: null
            restart: false
            stagger: 0 seconds
          memory:
            max-spill: false
            monitor-interval: 100ms
            pause: 0.8
            rebalance:
              measure: optimistic
              recipient-max: 0.6
              sender-min: 0.3
              sender-recipient-gap: 0.1
            recent-to-old-time: 30s
            spill: 0.7
            target: 0.6
            terminate: 0.95
            transfer: 0.1
          multiprocessing-method: spawn
          preload: []
          preload-argv: []
          profile:
            cycle: 1000ms
            enabled: false
            interval: 10ms
            low-level: false
          resources: {}
          transfer:
            message-bytes-limit: 50MB
          use-file-locking: true
          validate: false
      local_directory: /tmp
      optimization:
        annotations:
          fuse: true
        fuse:
          active: null
          ave-width: 1
          max-depth-new-edges: null
          max-height: .inf
          max-width: null
          rename-keys: true
          subgraphs: null
      scheduler: dask.distributed
      shuffle: tasks
      temporary-directory: null
      tokenize:
        ensure-deterministic: false
      visualization:
        engine: null
    constrained: []
    data: {}
    data_needed: {}
    executing: []
    has_what: {}
    id: Worker-c85b8f71-7ad0-4e08-bbfa-0f92dd6025af
    in_flight_tasks: []
    in_flight_workers: {}
    log: []
    logs:
    - - INFO
      - '2023-01-24 16:49:31,914 - distributed.worker - INFO -       Start worker
        at:      tcp://127.0.0.1:34489'
    - - INFO
      - '2023-01-24 16:49:31,914 - distributed.worker - INFO -          Listening
        to:      tcp://127.0.0.1:34489'
    - - INFO
      - '2023-01-24 16:49:31,914 - distributed.worker - INFO -           Worker name:                          0'
    - - INFO
      - '2023-01-24 16:49:31,914 - distributed.worker - INFO -          dashboard
        at:            127.0.0.1:43333'
    - - INFO
      - '2023-01-24 16:49:31,914 - distributed.worker - INFO - Waiting to connect
        to:      tcp://127.0.0.1:35679'
    - - INFO
      - 2023-01-24 16:49:31,915 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-24 16:49:31,915 - distributed.worker - INFO -               Threads:                          1'
    - - INFO
      - '2023-01-24 16:49:31,915 - distributed.worker - INFO -                Memory:                  15.25
        GiB'
    - - INFO
      - '2023-01-24 16:49:31,915 - distributed.worker - INFO -       Local Directory:
        /tmp/dask-worker-space/worker-qiaf9shy'
    - - INFO
      - 2023-01-24 16:49:31,915 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-24 16:49:31,915 - distributed.worker - INFO -       Start worker
        at:      tcp://127.0.0.1:46283'
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO -          Listening
        to:      tcp://127.0.0.1:46283'
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO -           Worker name:                          1'
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO -          dashboard
        at:            127.0.0.1:46415'
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO - Waiting to connect
        to:      tcp://127.0.0.1:35679'
    - - INFO
      - 2023-01-24 16:49:31,916 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO -               Threads:                          2'
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO -                Memory:                  15.25
        GiB'
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO -       Local Directory:
        /tmp/dask-worker-space/worker-44azq52g'
    - - INFO
      - 2023-01-24 16:49:31,916 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-24 16:49:31,925 - distributed.worker - INFO -         Registered
        to:      tcp://127.0.0.1:35679'
    - - INFO
      - 2023-01-24 16:49:31,925 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-24 16:49:31,926 - distributed.worker - INFO -         Registered
        to:      tcp://127.0.0.1:35679'
    - - INFO
      - 2023-01-24 16:49:31,926 - distributed.worker - INFO - -------------------------------------------------
    long_running: []
    max_spill: false
    memory_limit: 16371576832
    memory_monitor_interval: 0.1
    memory_pause_fraction: 0.8
    memory_spill_fraction: 0.7
    memory_target_fraction: 0.6
    missing_dep_flight: []
    nthreads: 1
    ready: []
    running: true
    scheduler: tcp://127.0.0.1:35679
    status: '<Status.running: ''running''>'
    stimulus_log: []
    tasks: {}
    thread_id: 139880511607424
    transfer_incoming_log: []
    transfer_outgoing_log: []
    transition_counter: 0
    type: Worker
  tcp://127.0.0.1:46283:
    address: tcp://127.0.0.1:46283
    busy_workers: []
    config:
      array:
        backend: numpy
        chunk-size: 128MiB
        rechunk-threshold: 4
        slicing:
          split-large-chunks: null
        svg:
          size: 120
      dataframe:
        backend: pandas
        dtype_backend: pandas
        parquet:
          metadata-task-size-local: 512
          metadata-task-size-remote: 16
        shuffle-compression: null
      distributed:
        adaptive:
          interval: 1s
          maximum: .inf
          minimum: 0
          target-duration: 5s
          wait-count: 3
        admin:
          event-loop: tornado
          log-format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          log-length: 10000
          max-error-length: 10000
          pdb-on-err: false
          system-monitor:
            disk: true
            host-cpu: false
            interval: 500ms
          tick:
            cycle: 1s
            interval: 500 ms
            limit: 3s
        client:
          heartbeat: 5s
          preload: []
          preload-argv: []
          scheduler-info-interval: 2s
          security-loader: null
        comm:
          compression: auto
          default-scheme: tcp
          offload: 10MiB
          recent-messages-log-length: 0
          require-encryption: null
          retry:
            count: 0
            delay:
              max: 20s
              min: 1s
          shard: 64MiB
          socket-backlog: 2048
          tcp:
            backend: tornado
          timeouts:
            connect: 5s
            tcp: 30s
          tls:
            ca-file: null
            ciphers: null
            client:
              cert: null
              key: null
            max-version: null
            min-version: 1.2
            scheduler:
              cert: null
              key: null
            worker:
              cert: null
              key: null
          ucx:
            create-cuda-context: null
            cuda-copy: null
            environment: {}
            infiniband: null
            nvlink: null
            rdmacm: null
            tcp: null
          websockets:
            shard: 8MiB
          zstd:
            level: 3
            threads: 0
        dashboard:
          export-tool: false
          graph-max-items: 5000
          link: '{scheme}://{host}:{port}/status'
          prometheus:
            namespace: dask
        deploy:
          cluster-repr-interval: 500ms
          lost-worker-timeout: 15s
        diagnostics:
          computations:
            ignore-modules:
            - distributed
            - dask
            - xarray
            - cudf
            - cuml
            - prefect
            - xgboost
            max-history: 100
          erred-tasks:
            max-history: 100
          nvml: true
        nanny:
          environ: {}
          pre-spawn-environ:
            MALLOC_TRIM_THRESHOLD_: 65536
            MKL_NUM_THREADS: 1
            OMP_NUM_THREADS: 1
            OPENBLAS_NUM_THREADS: 1
          preload: []
          preload-argv: []
        rmm:
          pool-size: null
        scheduler:
          active-memory-manager:
            interval: 2s
            measure: optimistic
            policies:
            - class: distributed.active_memory_manager.ReduceReplicas
            start: true
          allowed-failures: 3
          allowed-imports:
          - dask
          - distributed
          bandwidth: 100000000
          blocked-handlers: []
          contact-address: null
          dashboard:
            bokeh-application:
              allow_websocket_origin:
              - '*'
              check_unused_sessions_milliseconds: 500
              keep_alive_milliseconds: 500
            status:
              task-stream-length: 1000
            tasks:
              task-stream-length: 100000
            tls:
              ca-file: null
              cert: null
              key: null
          default-data-size: 1kiB
          default-task-durations:
            rechunk-split: 1us
            split-shuffle: 1us
          events-cleanup-delay: 1h
          events-log-length: 100000
          http:
            routes:
            - distributed.http.scheduler.prometheus
            - distributed.http.scheduler.info
            - distributed.http.scheduler.json
            - distributed.http.health
            - distributed.http.proxy
            - distributed.http.statics
          idle-timeout: null
          locks:
            lease-timeout: 30s
            lease-validation-interval: 10s
          pickle: true
          preload: []
          preload-argv: []
          transition-log-length: 100000
          unknown-task-duration: 500ms
          validate: false
          work-stealing: true
          work-stealing-interval: 100ms
          worker-saturation: 1.1
          worker-ttl: 5 minutes
        version: 2
        worker:
          blocked-handlers: []
          connections:
            incoming: 10
            outgoing: 50
          daemon: true
          http:
            routes:
            - distributed.http.worker.prometheus
            - distributed.http.health
            - distributed.http.statics
          lifetime:
            duration: null
            restart: false
            stagger: 0 seconds
          memory:
            max-spill: false
            monitor-interval: 100ms
            pause: 0.8
            rebalance:
              measure: optimistic
              recipient-max: 0.6
              sender-min: 0.3
              sender-recipient-gap: 0.1
            recent-to-old-time: 30s
            spill: 0.7
            target: 0.6
            terminate: 0.95
            transfer: 0.1
          multiprocessing-method: spawn
          preload: []
          preload-argv: []
          profile:
            cycle: 1000ms
            enabled: false
            interval: 10ms
            low-level: false
          resources: {}
          transfer:
            message-bytes-limit: 50MB
          use-file-locking: true
          validate: false
      local_directory: /tmp
      optimization:
        annotations:
          fuse: true
        fuse:
          active: null
          ave-width: 1
          max-depth-new-edges: null
          max-height: .inf
          max-width: null
          rename-keys: true
          subgraphs: null
      scheduler: dask.distributed
      shuffle: tasks
      temporary-directory: null
      tokenize:
        ensure-deterministic: false
      visualization:
        engine: null
    constrained: []
    data: {}
    data_needed: {}
    executing: []
    has_what: {}
    id: Worker-611ccaad-dacf-4f79-964f-e107a85b2406
    in_flight_tasks: []
    in_flight_workers: {}
    log: []
    logs:
    - - INFO
      - '2023-01-24 16:49:31,914 - distributed.worker - INFO -       Start worker
        at:      tcp://127.0.0.1:34489'
    - - INFO
      - '2023-01-24 16:49:31,914 - distributed.worker - INFO -          Listening
        to:      tcp://127.0.0.1:34489'
    - - INFO
      - '2023-01-24 16:49:31,914 - distributed.worker - INFO -           Worker name:                          0'
    - - INFO
      - '2023-01-24 16:49:31,914 - distributed.worker - INFO -          dashboard
        at:            127.0.0.1:43333'
    - - INFO
      - '2023-01-24 16:49:31,914 - distributed.worker - INFO - Waiting to connect
        to:      tcp://127.0.0.1:35679'
    - - INFO
      - 2023-01-24 16:49:31,915 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-24 16:49:31,915 - distributed.worker - INFO -               Threads:                          1'
    - - INFO
      - '2023-01-24 16:49:31,915 - distributed.worker - INFO -                Memory:                  15.25
        GiB'
    - - INFO
      - '2023-01-24 16:49:31,915 - distributed.worker - INFO -       Local Directory:
        /tmp/dask-worker-space/worker-qiaf9shy'
    - - INFO
      - 2023-01-24 16:49:31,915 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-24 16:49:31,915 - distributed.worker - INFO -       Start worker
        at:      tcp://127.0.0.1:46283'
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO -          Listening
        to:      tcp://127.0.0.1:46283'
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO -           Worker name:                          1'
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO -          dashboard
        at:            127.0.0.1:46415'
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO - Waiting to connect
        to:      tcp://127.0.0.1:35679'
    - - INFO
      - 2023-01-24 16:49:31,916 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO -               Threads:                          2'
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO -                Memory:                  15.25
        GiB'
    - - INFO
      - '2023-01-24 16:49:31,916 - distributed.worker - INFO -       Local Directory:
        /tmp/dask-worker-space/worker-44azq52g'
    - - INFO
      - 2023-01-24 16:49:31,916 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-24 16:49:31,925 - distributed.worker - INFO -         Registered
        to:      tcp://127.0.0.1:35679'
    - - INFO
      - 2023-01-24 16:49:31,925 - distributed.worker - INFO - -------------------------------------------------
    - - INFO
      - '2023-01-24 16:49:31,926 - distributed.worker - INFO -         Registered
        to:      tcp://127.0.0.1:35679'
    - - INFO
      - 2023-01-24 16:49:31,926 - distributed.worker - INFO - -------------------------------------------------
    long_running: []
    max_spill: false
    memory_limit: 16371576832
    memory_monitor_interval: 0.1
    memory_pause_fraction: 0.8
    memory_spill_fraction: 0.7
    memory_target_fraction: 0.6
    missing_dep_flight: []
    nthreads: 2
    ready: []
    running: true
    scheduler: tcp://127.0.0.1:35679
    status: '<Status.running: ''running''>'
    stimulus_log: []
    tasks: {}
    thread_id: 139880511607424
    transfer_incoming_log: []
    transfer_outgoing_log: []
    transition_counter: 0
    type: Worker
